{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단일 GPU, 또는 TPU 디바이스를 활용하여 Tunib의 KMWP 데이터셋을 skt-kogpt2-trinity 모델로 학습하는 Base-Code 입니다.  \n",
    "간단하게 주석을 적었으나, 곳곳에 넣은 링크에서 내용을 자세히 보는 것을 추천드립니다  \n",
    "처음 접하는 Huggingface가 당황스럽겠지만, 노드에서 배우던 그 BERT나 Transformer 모델을 단 몇줄의  \n",
    "코드로 사용할 수 있게 해주는 강력한 라이브러리 입니다  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프레임워크 및 라이브러리 임포트, TPU 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 07:40:09.736546: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:116] Libtpu path is: libtpu.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 07:40:11.772358: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-03 07:40:15.479637: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7a0daf0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-03 07:40:15.479667: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): TPU, 2a886c8\n",
      "2022-06-03 07:40:15.479674: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (1): TPU, 2a886c8\n",
      "2022-06-03 07:40:15.479679: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (2): TPU, 2a886c8\n",
      "2022-06-03 07:40:15.479683: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (3): TPU, 2a886c8\n",
      "2022-06-03 07:40:15.479688: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (4): TPU, 2a886c8\n",
      "2022-06-03 07:40:15.479692: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (5): TPU, 2a886c8\n",
      "2022-06-03 07:40:15.479697: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (6): TPU, 2a886c8\n",
      "2022-06-03 07:40:15.479701: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (7): TPU, 2a886c8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TFGPT2LMHeadModel, AutoConfig\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "tqdm.pandas() # tqdm 을 pandas 의 DataFrame 에 사용하기 위한 기본 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 및 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "data_path = '/home/dlfrnaos21/data'\n",
    "train_path = f'{data_path}/new_train.csv'\n",
    "test_path = f'{data_path}/test.csv'\n",
    "sample_path = f'{data_path}/sample_answersheet.json'\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample = pd.read_json(sample_path)\n",
    "\n",
    "# 기초 전처리 (tokenizer로 tokenize 후 decode하여 unk 토큰이 나오는 단어 변환)\n",
    "train.problem = train.problem.str.replace(\"캤\",\"캣\")\n",
    "train.problem = train.problem.str.replace(\"쨰\",\"째\")\n",
    "train.problem = train.problem.str.replace(\"츌\",\"출\")\n",
    "train.problem = train.problem.str.replace(\"찗\",\"짧\")\n",
    "train.problem = train.problem.str.replace(\"샙\",\"셉\")\n",
    "train.problem = train.problem.str.replace(\"땃\",\"땄\")\n",
    "train.code = train.code.str.replace(\"\\n\",\"&\")\n",
    "\n",
    "# Base tokenizer \n",
    "MODEL_PATH = 'skt/ko-gpt-trinity-1.2B-v0.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH,add_prefix_space=True,\n",
    "                                          bos_token=\"<s>\",\n",
    "                                                   eos_token=\"</s>\",\n",
    "                                                   unk_token=\"<unk>\",\n",
    "                                                   pad_token=\"<pad>\",\n",
    "                                                   mask_token=\"<mask>\",)\n",
    "# Code 문장 tokenize를 위한 code-t5 tokenizer\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8806/1527291977.py:5: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  train.c_joined = train.c_splited.str.replace(\"Ġ\",\"▁\")\n",
      "/tmp/ipykernel_8806/1527291977.py:6: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  train.splited_c = train.c_joined.str.split(\" \")\n"
     ]
    }
   ],
   "source": [
    "train['p_tokenized'] = train.problem.apply(lambda x: tokenizer.encode(x)) # problem to input_ids\n",
    "train['c_splited'] = train.code.apply(lambda x: code_tokenizer.tokenize(\"<s> \"+x+\" </s>\")) # code to splited tokens ex:) ['a', '=', '1',...]\n",
    "train.c_splited = train.c_splited.apply(lambda x: \" \".join(x)) # list[str] to string\n",
    "train.c_joined = train.c_splited.str.replace(\"Ġ\",\"▁\") # replace Ġ to ▁\n",
    "train.splited_c = train.c_joined.str.split(\" \") # split to list[str]\n",
    "train['c_tokenized'] = train.splited_c.apply(lambda x: tokenizer(x, is_split_into_words=True)['input_ids']) # list[str] to list[int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2820.000000\n",
       "mean       74.078723\n",
       "std        29.936687\n",
       "min        29.000000\n",
       "25%        53.750000\n",
       "50%        65.500000\n",
       "75%        85.000000\n",
       "max       234.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tokenized = train.p_tokenized + train.c_tokenized # add whole tokens\n",
    "length = full_tokenized.apply(len) # length check\n",
    "length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN을 줄이고 BATCH_SIZE를 높이거나, MAX_LEN을 높이고 BATCH_SIZE를 줄이거나 선택\n",
    "MAX_LEN = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN 이하의 문장만 데이터로 사용합니다\n",
    "cut_index = length[length <= MAX_LEN]\n",
    "cut_text_list = full_tokenized[length <= MAX_LEN].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DataCollatorForLanguageModeling](https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)  \n",
    "language modeling에 사용하는 기능으로 전체 길이에 맞게 input_ids에 padding하며, label을 함께 반환합니다  \n",
    "GPT는 Causal language modeling을 적용하므로 masked language modeling인 mlm기능은 끄고, input_ids와 동일하나, 패딩만 -100으로 된 label을 반환합니다  \n",
    "자세한 내용은 huggingface의 [코스](https://huggingface.co/course/en/chapter7/6?fw=tf)를 참고하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors='tf') # get data_collator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return type {\"input_ids\":[input_ids],\"labels\":[labels]}\n",
    "collated_data = data_collator([cut_text_list[i] for i in range(len(cut_text_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 마스크를 직접 만듭니다. input_ids를 기준으로 3(padding token)이 아닌곳은 1을 넣습니다\n",
    "attention_mask = []\n",
    "for i in collated_data['input_ids']:\n",
    "    attention_mask.append([1 if token != 3 else 0 for token in i ])\n",
    "collated_data['attention_mask'] = tf.convert_to_tensor(attention_mask,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type 지정을 위한 tf.cast\n",
    "padded_cut_text_list = tf.cast(collated_data['input_ids'],dtype=tf.int32)\n",
    "attention_mask = tf.cast(collated_data['attention_mask'],dtype=tf.int32)\n",
    "labels = tf.cast(collated_data['labels'],dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2340,), (261,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 클래스의 분포에 맞게 train과 valid index를 1회만 추출\n",
    "train_idx, valid_idx = next(iter(StratifiedKFold(n_splits=10, shuffle=True, random_state=5959).split(train.problem[cut_index], train['class'][cut_index])))\n",
    "train_idx.shape, valid_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data에서 각 train, valid 에 맞게 데이터를 인덱싱 합니다\n",
    "train_set = dict(\n",
    "    input_ids=padded_cut_text_list.numpy()[train_idx],\n",
    "    attention_mask = attention_mask.numpy()[train_idx],\n",
    "    labels=labels.numpy()[train_idx],\n",
    ")\n",
    "valid_set = dict(\n",
    "    input_ids=padded_cut_text_list.numpy()[valid_idx],\n",
    "    attention_mask=attention_mask.numpy()[valid_idx],\n",
    "    labels=labels.numpy()[valid_idx]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 07:41:27.174125: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-03 07:41:27.196530: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-03 07:41:27.300416: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-03 07:41:29.985829: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:167] Warning: Using tf.random.truncated_normal with XLA compilation will ignore seeds; consider using tf.random.stateless_truncated_normal instead if reproducible behavior is desired. TruncatedNormal\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.23.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'lm_head.weight', 'transformer.h.7.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.14.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_PATH, n_ctx=MAX_LEN, \n",
    "                                                   bos_token_id=tokenizer.bos_token_id, \n",
    "                                                   eos_token_id=tokenizer.eos_token_id,\n",
    "                                                   pad_token_id=tokenizer.pad_token_id,\n",
    "                                                    )\n",
    "model = TFGPT2LMHeadModel.from_pretrained(MODEL_PATH, config=config, from_pt=True)\n",
    "optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "model(model.dummy_inputs)\n",
    "# huggingface 모델의 특징으로, 모델 안에 loss function이 내장되어 있습니다 따라서 따로 설정하지 않아도 loss가 나옵니다\n",
    "model.compile(optimizer=optimizer,)\n",
    "\n",
    "# Earlystop 조건으로 자유롭게 바꿀 수 있습니다\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    ")\n",
    "\n",
    "# float32, float16 자료구조를 mix하여 연산하는 것으로 효율적인 연산을 하나, 결과값에 영향이 있을 수 있습니다\n",
    "# Tensorflow Document 참고 https://www.tensorflow.org/guide/mixed_precision?hl=ko\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 입력 파이프라인을 최적화 하는 과정\n",
    "# 왜 해야하는지 궁금하다면 https://www.tensorflow.org/guide/data_performance?hl=ko\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "def prefetch_ds(ds):\n",
    "    return ds.repeat().shuffle(2300).batch(BATCH_SIZE,num_parallel_calls=AUTO).prefetch(AUTO)\n",
    "\n",
    "tf_train_ds = tf.data.Dataset.from_tensor_slices(train_set)\n",
    "tf_train_ds = prefetch_ds(tf_train_ds)\n",
    "tf_val_ds = tf.data.Dataset.from_tensor_slices(valid_set)\n",
    "tf_val_ds = prefetch_ds(tf_val_ds)\n",
    "\n",
    "train_step = len(train_set['input_ids']) // BATCH_SIZE\n",
    "val_step = len(valid_set['input_ids']) // BATCH_SIZE\n",
    "callbacks = [es]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 07:43:03.494078: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:263] Subgraph fingerprint:16662598172746951624\n",
      "2022-06-03 07:43:06.211046: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-06-03 07:43:08.293929: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-06-03 07:43:11.392951: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:435] TPU host compilation cache miss: cache_key(1509945416255256015), session_name()\n",
      "2022-06-03 07:44:41.748193: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:180] Compilation of 1509945416255256015 with session name  took 1m30.355153419s and succeeded\n",
      "2022-06-03 07:44:41.881974: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:468] TPU host compilation cache: compilation complete for cache_key(1509945416255256015), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_train_function_16662598172746951624\", property.function_library_fingerprint = 10798428425075052016, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, topology.MissingChipCount() = 0, std::string(property.shapes_prefix) = \"16,128,;16,128,;16,128,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2022-06-03 07:44:41.882027: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:534] After adding entry for key 1509945416255256015 with session_name  cache is 1 entries (439367888 bytes),  marked for eviction 0 entries (0 bytes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 2.0066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 07:44:58.214776: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:263] Subgraph fingerprint:11236983988290291293\n",
      "2022-06-03 07:44:58.818413: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-06-03 07:44:59.380756: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-06-03 07:45:00.176891: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:435] TPU host compilation cache miss: cache_key(10690799761149190951), session_name()\n",
      "2022-06-03 07:45:18.444485: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:180] Compilation of 10690799761149190951 with session name  took 18.267515397s and succeeded\n",
      "2022-06-03 07:45:18.482488: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:468] TPU host compilation cache: compilation complete for cache_key(10690799761149190951), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_test_function_11236983988290291293\", property.function_library_fingerprint = 2806801716922167655, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, topology.MissingChipCount() = 0, std::string(property.shapes_prefix) = \"16,128,;16,128,;16,128,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2022-06-03 07:45:18.482540: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:534] After adding entry for key 10690799761149190951 with session_name  cache is 2 entries (552376236 bytes),  marked for eviction 0 entries (0 bytes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 182s 2s/step - loss: 2.0066 - val_loss: 1.4131\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 1.3196 - val_loss: 1.1490\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 1.1385 - val_loss: 1.1267\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 1.0351 - val_loss: 1.0382\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 12s 683ms/step - loss: 0.9671 - val_loss: 0.9841\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.9119 - val_loss: 0.9966\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.8494 - val_loss: 0.9767\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 12s 683ms/step - loss: 0.8084 - val_loss: 0.9709\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.7564 - val_loss: 0.9704\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.7277 - val_loss: 0.9093\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.6882 - val_loss: 0.9359\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.6602 - val_loss: 0.8395\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.6218 - val_loss: 0.8835\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.5902 - val_loss: 0.8491\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.5597 - val_loss: 0.9358\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dist_train_ds,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=dist_val_ds,\n",
    "    steps_per_epoch=train_step,\n",
    "    validation_steps=val_step,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set 예측 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.problem = test.problem.str.replace(\"캤\",\"캣\")\n",
    "test.problem = test.problem.str.replace(\"쨰\",\"째\")\n",
    "test.problem = test.problem.str.replace(\"츌\",\"출\")\n",
    "test.problem = test.problem.str.replace(\"찗\",\"짧\")\n",
    "test.problem = test.problem.str.replace(\"샙\",\"셉\")\n",
    "test.problem = test.problem.str.replace(\"땃\",\"땄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(test.problem.to_list(), padding=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "tf_test_ds = tf.data.Dataset.from_tensor_slices(input_ids['input_ids'])\n",
    "tf_test_ds = tf_test_ds.batch(BATCH_SIZE, num_parallel_calls=AUTO).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 79), dtype=int32, numpy=\n",
       "array([[30708, 42034, 30206, ...,     3,     3,     3],\n",
       "       [33104, 19044, 36686, ...,     3,     3,     3],\n",
       "       [34424, 18792, 30239, ...,     3,     3,     3],\n",
       "       ...,\n",
       "       [34657, 30991, 32410, ...,     3,     3,     3],\n",
       "       [31787, 42501,   565, ...,     3,     3,     3],\n",
       "       [30473, 29994, 33781, ...,     3,     3,     3]], dtype=int32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tf_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in tf_test_ds:\n",
    "      output = model.generate(i, max_length=128)\n",
    "      decoded.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['윗접시저울의 왼쪽 접시에 무게가 671g 인 소설책 한 권과 450g 인 과학책 한 권을 올려놓고 오른쪽 접시에 38g 짜리 분동 18개를 올려놓았더니 윗접시저울이 왼 쪽으로 기울었습니다. 윗접시저울이 수평이 되려면 1g 짜리 분동이 몇 개 필요한지 구해 보세요. a = 67 1 & b = 4 50 & c = 38 & d = 18 & y = ( a + b ) // c & print ( y ) ',\n",
       " '한 자리 수 A, B가 두 자리 수끼리의 덧셈 식 5A + B2 = 78 를 만족할 때, 두 수 A, B의 합을 구하세요.                        ',\n",
       " '한 시간에 오토바이를 15대씩 만드는 공장이 있습니다. 이 공장에서 21시간 동안 만들 수 있는 오토바이는 모두 몇 대가 될까요? 5년간 만들 수 있는 오토바이는 모두 몇 대일까요? ',\n",
       " '어떤 두 수의 최대공약수가 64일 때 두 수의 공약수는 모두 몇 개일까요? 앞으로도 ',\n",
       " '약분하면 3/4가 되는 분수 중에서 분모와 분자의 차가 12일 때, 분모와 분자의 합을 구해 보세요.  앞으로도 분모와 분자의 차는 12로 일정합니다.  과 같이 분모와 분자의 차를 구하시오. a = 3 / 4 & b = 12 & y = a * b & print (\" { :. 2 f }',\n",
       " '할머니께서 쪽파를 35개 사 오셨습니다. 이 쪽파을 일주일 동안 매일 같은 수만큼 먹으려고 합니다. 하루에 몇 개씩 먹어야 합니까? 5년간 매일 쪽파를 먹으려면 일주일 동안 쪽파를 몇 개씩 먹어야 합니까? a = 35 & b = 7 & y = a // b & print ( y ) ',\n",
       " '어떤 수에서 6를뺀 후 3을 더해야 할 것을 잘못하여 어떤 수에 5를 더한 후 4을 빼었더니 9이 되었습니다. 바르게 계산하면 얼마일까요? 앞으로도 잘못하여 뺐던 수를 구하시오. a = 6 & b = 3 & c = 5 & d = 4 & e = 9 & y = ( e // d + c ) * a & print ( y',\n",
       " '4 장의 수 카드 3, 8, 7, 4 중 3 장을 골라 한 번씩만 사용하여 만들 수 있는 가장 큰 세 자리 수와 남은 수 카드의 수의 곱은 얼마일까요? 5년간 만들 수 있는 가장 큰 세 자리 수와 남은 수 카드의 수의 곱은 얼마일까요? ',\n",
       " '883.22보다 12.86 작은 값을 구해보세요. 해결해 보세요. a = 8 83. 22 & b = 12. 86 & c = b - a & y = a - c & print ( y ) ',\n",
       " '혜주는 우유를 한 번에 320mL씩 하루 4번 마십니다. 5일 동안 마신 우유의 양은 모두 몇 mL일까요? 5년간 마신 우유의 양은 모두 몇 mL일까요? a = 3 20 & b = 5 & y = a * b & print ( y ) ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface model hub를 활용하면 쉽게 모델을 저장하고 불러올 수 있습니다  \n",
    "단 사전에 huggingface에 가입해야하며, 가입한 아이디의 API 토큰을 가져와서 login시에 뜨는 빈칸에  \n",
    "토큰을 넣어줘야 작동합니다  \n",
    "관련 내용은 [huggingface course 4](https://huggingface.co/course/en/chapter4/1?fw=tf)를 참고해주세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/madatnlp/not_class_trinity-kormath-128\n",
      "   d05779d..81f1494  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/madatnlp/not_class_trinity-kormath-128\n",
      "   d05779d..81f1494  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cdbb2550254e48bd7e3ca491f5e4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tf_model.h5:   0%|          | 32.0k/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/madatnlp/not_class_trinity-kormath-128\n",
      "   81f1494..03e1df2  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/madatnlp/not_class_trinity-kormath-128\n",
      "   81f1494..03e1df2  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 적절한 repo name과 organization_name을 설정해주세요\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "tokenizer.save_pretrained(\"repository_name\",push_to_hub=True,organization=\"organization_name\")\n",
    "model.save_pretrained(\"repository_name\",push_to_hub=True,organization=\"organization_name\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
